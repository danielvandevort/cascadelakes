{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T22:26:12.348661Z",
     "start_time": "2025-09-02T22:26:12.344111Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# imports and setup\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, classification_report\n",
    "\n",
    "# USER_INPUT is noted where the user is required to edit for their own use.\n",
    "\n",
    "final = r'USER_INPUT' # working data directory"
   ],
   "id": "ca5ab8f0d109e0f5",
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-02T20:53:21.113150Z",
     "start_time": "2025-09-02T20:53:21.108583Z"
    }
   },
   "source": [
    "# Define Winter DOY Function\n",
    "def calculate_winter_doy(date):\n",
    "    if pd.isna(date):  # Handle NaT values\n",
    "        return None\n",
    "    if date.month >= 10:  # From October to December (current year)\n",
    "        return date.dayofyear - 273  # October 1 = 0 (non-leap year)\n",
    "    else:  # From January to April (next year)\n",
    "        return date.dayofyear + 92  # Add days from previous year's winter"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T22:21:46.455979Z",
     "start_time": "2025-09-02T22:10:28.520220Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Prepare dataset\n",
    "training_data = []\n",
    "all_lake_data = {}  # Will store {lake_id: {winter_year: {week: weekly_afdd_value}}}\n",
    "for f in tqdm([i for i in os.listdir(final)], desc='Building dataset...', unit='file'):\n",
    "    fp = os.path.join(final, f)\n",
    "    lake_data = pd.read_csv(fp)\n",
    "    lake_data['date'] = pd.to_datetime(lake_data['date'])\n",
    "    # Calculate winter DOY\n",
    "    lake_data['winter_doy'] = lake_data['date'].apply(calculate_winter_doy)\n",
    "    # Calculate winter year\n",
    "    lake_data[\"winter_year\"] = lake_data['date'].apply(\n",
    "        lambda d: d.year if d.month >= 10 else d.year - 1)\n",
    "    # Create ice binary variable (1 for ice, 0 for open water)\n",
    "    lake_data['ice_binary'] = ((lake_data['ice_class'] == 'full') |  # pipe is OR\n",
    "                               (lake_data['ice_class'] == 'partial')).astype(int)\n",
    "    # Get lake properties\n",
    "    lake_id = int(f.split('_')[1].split('.')[0])\n",
    "    # Assign week number (relative to winter year) using winter DOY\n",
    "    lake_data['winter_week'] = lake_data['winter_doy'] // 7\n",
    "    # Calculate weekly temperature averages and AFDD\n",
    "    lake_data['tmean_C'] = (lake_data['tmin'] + lake_data['tmax']) / 2\n",
    "    lake_data['freezing_degrees'] = lake_data['tmean_C'].apply(lambda x: max(0 - x, 0))\n",
    "    lake_data['afdd'] = lake_data.groupby('winter_year')['freezing_degrees'].cumsum()\n",
    "    weekly_avg_afdd = lake_data.groupby(['winter_year', 'winter_week'])['afdd'].mean().reset_index()\n",
    "    weekly_avg_afdd.rename(columns={'afdd': 'weekly_afdd'}, inplace=True)\n",
    "    lake_data = pd.merge(lake_data, weekly_avg_afdd, on=['winter_year', 'winter_week'], how='left')\n",
    "    weekly_avg_low = lake_data.groupby(['winter_year', 'winter_week'])['tmin'].mean().reset_index()\n",
    "    weekly_avg_low.rename(columns={'tmin': 'weekly_mean_low'}, inplace=True)\n",
    "    # Merge weekly AFDD back to the lake_data dataframe\n",
    "    lake_data = lake_data.merge(weekly_avg_low, on=['winter_year', 'winter_week'], how='left')\n",
    "    if lake_id not in all_lake_data: # at this point in the code, it won't be, so this conditional adds it in to the higher dictionary.\n",
    "        all_lake_data[lake_id] = {}\n",
    "    for _, row in lake_data.iterrows():\n",
    "        year = row['winter_year']\n",
    "        week = row['winter_week'] # starts at 0\n",
    "        afdd7 = row['afdd'] # same\n",
    "        wk_mean_low = row['weekly_mean_low'] # same\n",
    "        if year not in all_lake_data[lake_id]:\n",
    "            all_lake_data[lake_id][year] = {}\n",
    "        all_lake_data[lake_id][year][week] = {\n",
    "            'afdd': afdd7,\n",
    "            'mean_low': wk_mean_low\n",
    "        }\n",
    "    # Only include rows with valid ice classification\n",
    "    valid_rows = lake_data[~pd.isna(lake_data['ice_class'])].copy()\n",
    "    # Add each valid row to the training data\n",
    "    for _, row in valid_rows.iterrows():\n",
    "        training_data.append({  # misleading name ....\n",
    "            'elevation': row['Elevation_m'],\n",
    "            'latitude': row['Latitude'],\n",
    "            'afdd': row['afdd'],\n",
    "            'area': row['Area_sqkm'],\n",
    "            '$ T_{wk,L} $': row['weekly_mean_low'],\n",
    "            'ice_binary': row['ice_binary'], # target\n",
    "            'winter_year': row['winter_year'],\n",
    "            'lake_id': lake_id,\n",
    "            'winter_doy': row['winter_doy'],\n",
    "        })\n",
    "# Convert to dataframe\n",
    "ice_df = pd.DataFrame(training_data)\n",
    "# Handle any NaNs that might remain\n",
    "ice_df = ice_df.dropna()\n"
   ],
   "id": "916f5f2d8d865ea7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Building dataset...:   0%|          | 0/2872 [00:00<?, ?file/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cabe78f7430447c58516194a23616346"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### The following 2 cells were used to estimate the missing temperature data for Jan-Jun 2024. The Daymet temperature data set has since been updated to include these months, so this section is no longer necessary. However, it is retained here for reference.",
   "id": "50b82c53ec5c3f8e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T22:21:56.187049Z",
     "start_time": "2025-09-02T22:21:56.180755Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# def estimate_2024_temp_values(lake_id, winter_doy, all_lake_data):\n",
    "#     \"\"\"Estimate temperature values for 2024 based on historical patterns, including AFDD.\"\"\"\n",
    "#     week = winter_doy // 7\n",
    "#     values_low = []\n",
    "#     values_afdd = []\n",
    "#     # Loop over historical years for this lake\n",
    "#     if lake_id in all_lake_data:\n",
    "#         for year in all_lake_data[lake_id]:\n",
    "#             if year < 2023 and week in all_lake_data[lake_id][year]:  # Only use past years\n",
    "#                 values_low.append(all_lake_data[lake_id][year][week]['mean_low'])\n",
    "#                 values_afdd.append(all_lake_data[lake_id][year][week]['afdd'])\n",
    "#     # Return the mean, or np.nan if no values found\n",
    "#     return (\n",
    "#         float(np.mean(values_low)) if values_low else np.nan,\n",
    "#         float(np.mean(values_afdd)) if values_afdd else np.nan\n",
    "#     )"
   ],
   "id": "96efc6609136aa27",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T22:21:56.376978Z",
     "start_time": "2025-09-02T22:21:56.370989Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# def get_weekly_temp_values(lake_id, year, winter_doy, all_lake_data):\n",
    "#     \"\"\"\n",
    "#     Get weekly temperature values for a given lake, year, and winter day.\n",
    "#     Returns mean_low, and afdd, with a fallback to default or averaged values\n",
    "#     if specific data is unavailable.\n",
    "#     \"\"\"\n",
    "#     week = winter_doy // 7\n",
    "#     # Try to get the value for the exact week\n",
    "#     if lake_id in all_lake_data and year in all_lake_data[lake_id] and week in all_lake_data[lake_id][year]:\n",
    "#         return (\n",
    "#         all_lake_data[lake_id][year][week]['mean_low'],\n",
    "#         all_lake_data[lake_id][year][week]['afdd']\n",
    "#     )\n",
    "#     else:\n",
    "#         return None"
   ],
   "id": "cb2052732fa4a729",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Create Complete Dataset for Analysis",
   "id": "b83899e3f6e27dda"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T22:25:19.539811Z",
     "start_time": "2025-09-02T22:22:00.588514Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create a complete date range for all lakes and winters\n",
    "all_lakes = ice_df['lake_id'].unique()\n",
    "all_winters = range(ice_df['winter_year'].min(), ice_df['winter_year'].max() + 1) # +1 because `range` doesn't include the last year by default\n",
    "# Should now account for May-June\n",
    "print(\"\\nGenerating complete dataset with gap filling...\")\n",
    "# Get STATIC lake properties for each lake\n",
    "lake_properties = ice_df.groupby('lake_id').agg({\n",
    "    'elevation': 'first',\n",
    "    'latitude': 'first',\n",
    "    'area': 'first',\n",
    "}).reset_index()\n",
    "\n",
    "complete_grid = []  # Create empty list for full grid\n",
    "winter_2023_lakes = set()  # Track which lake-years need special handling\n",
    "for _, lake in tqdm(lake_properties.iterrows(), total=len(lake_properties), desc='Processing lakes'):\n",
    "    for year in all_winters:\n",
    "        # Get data for this lake and winter\n",
    "        lake_year_data = ice_df[(ice_df['lake_id'] == lake['lake_id']) &\n",
    "                                (ice_df['winter_year'] == year)]\n",
    "        # Special handling for 2024\n",
    "        is_winter_2023 = (year == 2023)\n",
    "        if is_winter_2023:\n",
    "            winter_2023_lakes.add(lake['lake_id'])\n",
    "        if not lake_year_data.empty:\n",
    "            # Create lookup dictionaries\n",
    "            afdd_lookup = dict(zip(lake_year_data['winter_doy'], lake_year_data['afdd']))\n",
    "            low_temp_lookup = dict(zip(lake_year_data['winter_doy'], lake_year_data['$ T_{wk,L} $']))\n",
    "            ice_lookup = dict(zip(lake_year_data['winter_doy'], lake_year_data['ice_binary']))\n",
    "            is_estimated = False\n",
    "            for day in range(1, 272): # does not account for leap years. This establishes the rolling weeks.\n",
    "                # For 2024, handle the Jan-Jun gap (winter_doy >= 92)\n",
    "                if is_winter_2023 and day >= 92: # This is the IS DOY\n",
    "                    # Use estimates for 2024 based on climatological averages\n",
    "                    low_temp, afdd_value = estimate_2024_temp_values(\n",
    "                        lake['lake_id'], day, all_lake_data\n",
    "                    )\n",
    "                    is_estimated = True\n",
    "                else:\n",
    "                    # Get values from lookups or calculate from weekly averages\n",
    "                    afdd_value = afdd_lookup.get(day)\n",
    "                    low_temp = low_temp_lookup.get(day)\n",
    "\n",
    "                    # If missing, get from weekly data...\n",
    "                    if low_temp is None:\n",
    "                        low_temp, afdd_value = get_weekly_temp_values(lake['lake_id'], year, day, all_lake_data)\n",
    "                # Add entry to complete grid\n",
    "                complete_grid.append({\n",
    "                    'lake_id': lake['lake_id'],\n",
    "                    'winter_year': year,\n",
    "                    'winter_doy': day, #\n",
    "                    'elevation': lake['elevation'],\n",
    "                    'latitude': lake['latitude'],\n",
    "                    'area': lake['area'],\n",
    "                    '$ T_{wk,L} $': low_temp,\n",
    "                    'afdd': afdd_value,\n",
    "                    'is_2024_estimate': 1 if is_estimated else 0,\n",
    "                    'data_type': 'Observed' if day in ice_lookup else 'Modeled',\n",
    "                    'ice_binary': ice_lookup.get(day)  # Will be None for days we need to predict\n",
    "                })\n",
    "\n",
    "# Convert to dataframe\n",
    "complete_df = pd.DataFrame(complete_grid)\n",
    "# Print info about the 2024 gap\n",
    "print(f\"\\nFilled data for Jan-Apr 2024 for {len(winter_2023_lakes)} lakes using climatological averages\")"
   ],
   "id": "57d45f9ff77ff001",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating complete dataset with gap filling...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Processing lakes:   0%|          | 0/2819 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "047ecbcc16ec43e0bde375f86f1a0a43"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Filled data for Jan-Apr 2024 for 2819 lakes using climatological averages\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model Training and Testing: Cross-Validation",
   "id": "febaea56bb2836fb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T22:27:55.722722Z",
     "start_time": "2025-09-02T22:27:55.709765Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Cross-validation cell\n",
    "#################### THIS IS THE FEATURE SET ###############################\n",
    "X = ice_df[['elevation', 'latitude', 'area', 'afdd', '$ T_{wk,L} $']]\n",
    "############################################################################\n",
    "y = ice_df['ice_binary'] # Target"
   ],
   "id": "de4d0c6c203d91eb",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T22:27:59.199709Z",
     "start_time": "2025-09-02T22:27:57.643160Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Use lake_id as the grouping variable for cross-validation\n",
    "group_kfold = GroupKFold(n_splits=5) # it was found that increasing fold numbers did not increase performance and increased computational cost. This is why we are using 5 folds.\n",
    "cv_scores = []\n",
    "cv_auc_scores = []\n",
    "cv_confusion_matrices = []\n",
    "\n",
    "print(\"\\nPerforming lake-based cross-validation...\")\n",
    "for i, (train_idx, test_idx) in enumerate(group_kfold.split(X, y, groups=ice_df['lake_id'])):\n",
    "    # Split data\n",
    "    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "    # Standardize\n",
    "    scaler = StandardScaler() # NEED THIS INSTANCE\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    # Train model\n",
    "    log_reg = LogisticRegression(max_iter=100, random_state=42, verbose = 1) # default solver lbfgs\n",
    "    log_reg.fit(X_train_scaled, y_train)\n",
    "    # Test model\n",
    "    y_pred = log_reg.predict(X_test_scaled)\n",
    "    y_prob = log_reg.predict_proba(X_test_scaled)[:, 1]\n",
    "    # Calculate metrics\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_prob)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    cv_scores.append(acc)\n",
    "    cv_auc_scores.append(auc)\n",
    "    cv_confusion_matrices.append(cm)\n",
    "    print(f\"Fold {i + 1} split: {len(train_idx)} training samples, {len(test_idx)} testing samples\")\n",
    "    print(f\"Percentage split: {len(train_idx) / len(X) * 100:.1f}% train, {len(test_idx) / len(X) * 100:.1f}% test\")\n",
    "\n",
    "\n",
    "print(f\"\\nCross-validation results:\")\n",
    "print(f\"Accuracy: {np.mean(cv_scores):.4f} ± {np.std(cv_scores):.4f}\")\n",
    "print(f\"AUC: {np.mean(cv_auc_scores):.4f} ± {np.std(cv_auc_scores):.4f}\")\n",
    "# Average confusion matrix across folds\n",
    "avg_cm = np.mean(cv_confusion_matrices, axis=0)\n",
    "print(\"\\nAverage Confusion Matrix:\")\n",
    "print(avg_cm)\n"
   ],
   "id": "eaf5f8b705fd920d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performing lake-based cross-validation...\n",
      "Fold 1 split: 512926 training samples, 128218 testing samples\n",
      "Percentage split: 80.0% train, 20.0% test\n",
      "Fold 2 split: 512900 training samples, 128244 testing samples\n",
      "Percentage split: 80.0% train, 20.0% test\n",
      "Fold 3 split: 512896 training samples, 128248 testing samples\n",
      "Percentage split: 80.0% train, 20.0% test\n",
      "Fold 4 split: 512928 training samples, 128216 testing samples\n",
      "Percentage split: 80.0% train, 20.0% test\n",
      "Fold 5 split: 512926 training samples, 128218 testing samples\n",
      "Percentage split: 80.0% train, 20.0% test\n",
      "\n",
      "Cross-validation results:\n",
      "Accuracy: 0.8010 ± 0.0046\n",
      "AUC: 0.8780 ± 0.0038\n",
      "\n",
      "Average Confusion Matrix:\n",
      "[[66750.8  9976. ]\n",
      " [15540.4 35961.6]]\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model Training and Testing: Time-based Holdouts",
   "id": "46094de8dce41b51"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T22:28:01.555354Z",
     "start_time": "2025-09-02T22:28:01.001649Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"\\nTraining final model...\")\n",
    "X_all_scaled = scaler.fit_transform(X) # this is the training data\n",
    "lake_ice_model = LogisticRegression(max_iter=100, random_state=42)\n",
    "lake_ice_model.fit(X_all_scaled, y)\n",
    "\n",
    "# Examine coefficients\n",
    "feature_names = ['elevation', 'latitude', 'area', 'afdd', '$ T_{wk,L} $']\n",
    "coefficients = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Coefficient': lake_ice_model.coef_[0],\n",
    "    'Abs_Coefficient': np.abs(lake_ice_model.coef_[0])\n",
    "})\n",
    "coefficients = coefficients.sort_values('Abs_Coefficient', ascending=False)\n",
    "print(\"\\nModel Coefficients (sorted by importance):\")\n",
    "print(coefficients)\n",
    "\n",
    "# Create a time-based holdout set for final validation (optional but recommended)\n",
    "X_time_train = X[ice_df['winter_year'] < 2022]\n",
    "y_time_train = y[ice_df['winter_year'] < 2022]\n",
    "X_time_test = X[ice_df['winter_year'] >= 2022]\n",
    "y_time_test = y[ice_df['winter_year'] >= 2022]\n",
    "\n",
    "X_time_train_scaled = scaler.fit_transform(X_time_train)\n",
    "X_time_test_scaled = scaler.transform(X_time_test)\n",
    "\n",
    "time_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "time_model.fit(X_time_train_scaled, y_time_train)\n",
    "\n",
    "y_time_pred = time_model.predict(X_time_test_scaled)\n",
    "y_time_prob = time_model.predict_proba(X_time_test_scaled)[:, 1]\n"
   ],
   "id": "2d5f13a6c40282f4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training final model on all data...\n",
      "\n",
      "Model Coefficients (sorted by importance):\n",
      "        Feature  Coefficient  Abs_Coefficient\n",
      "4  $ T_{wk,L} $    -1.401267         1.401267\n",
      "1      latitude     0.900415         0.900415\n",
      "3          afdd     0.721896         0.721896\n",
      "0     elevation     0.645643         0.645643\n",
      "2          area    -0.075735         0.075735\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## The following 2 cells run the model on a specific lake and winter season to visualize performance. This is useful for model diagnostics and understanding how well the model captures seasonal ice dynamics.",
   "id": "726fda4c8b8f0aa7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-03T00:15:25.578326Z",
     "start_time": "2025-09-03T00:15:25.434630Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # Choose a specific lake and winter season\n",
    "# lake_id = complete_df['lake_id'].unique()[0]  # You can change to any lake ID\n",
    "# winter_year = 2022  # Choose a year with good observation coverage\n",
    "#\n",
    "# # Filter data for the selected lake and winter\n",
    "# lake_season = complete_df[(complete_df['lake_id'] == lake_id) &\n",
    "#                           (complete_df['winter_year'] == winter_year)].copy()\n",
    "#\n",
    "# # Sort by winter_doy\n",
    "# lake_season = lake_season.sort_values('winter_doy')\n",
    "#\n",
    "# # Create dates for x-axis (convert winter_doy to actual dates)\n",
    "# start_date = pd.Timestamp(f'{winter_year}-10-01')  # Winter day 0 = October 1\n",
    "# lake_season['date'] = lake_season['winter_doy'].apply(\n",
    "#     lambda x: start_date + pd.Timedelta(days=int(x)))\n",
    "#\n",
    "# # Create a new column for model predictions on ALL days (including observation days)\n",
    "# # We need to re-run predictions on observation days\n",
    "# observed_rows = lake_season[lake_season['data_type'] == 'Observed']\n",
    "#\n",
    "# # Extract features for all rows (both observed and unobserved)\n",
    "# X_features = lake_season[['elevation', 'latitude', 'area', 'afdd', '$ T_{wk,L} $']]\n",
    "# valid_rows = ~X_features.isna().any(axis=1)\n",
    "# X_valid = X_features[valid_rows]\n",
    "#\n",
    "# # Scale and make predictions\n",
    "# X_scaled = scaler.transform(X_valid)\n",
    "# all_predictions = lake_ice_model.predict_proba(X_scaled)[:, 1]\n",
    "#\n",
    "# # Store predictions in a new column\n",
    "# lake_season.loc[valid_rows, 'true_model_prediction'] = all_predictions\n",
    "#\n"
   ],
   "id": "81dcdd922a64f62f",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T17:28:09.381643Z",
     "start_time": "2025-10-27T17:28:09.374535Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # Create the figure\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# ax = plt.gca()  # Get current axes\n",
    "# ax.set_facecolor('w')\n",
    "# ax.spines['left'].set_visible(True)\n",
    "# ax.spines['left'].set_linewidth(1.0)\n",
    "# ax.spines['left'].set_color('black')\n",
    "# ax.spines['bottom'].set_visible(True)\n",
    "# ax.spines['bottom'].set_linewidth(1.0)\n",
    "# ax.spines['bottom'].set_color('black')\n",
    "# ax.spines['right'].set_visible(False)\n",
    "# ax.spines['top'].set_visible(False)\n",
    "# ax.tick_params(axis='both', which='major', direction='inout', width = 2, length = 10, left=True, bottom = True, labelsize=16)\n",
    "# # Plot model predictions as a continuous line\n",
    "# plt.plot(lake_season['date'], lake_season['true_model_prediction'],\n",
    "#          color='k', linestyle='-', linewidth=2, label='Model Prediction')\n",
    "#\n",
    "# # Plot actual observations as points\n",
    "# observed = lake_season[lake_season['data_type'] == 'Observed']\n",
    "# plt.scatter(observed['date'], observed['ice_binary'],\n",
    "#             color='k', marker = 'x', s=42, label='Observations', zorder=5)\n",
    "#\n",
    "# # Add threshold line\n",
    "# plt.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5, label='Decision Threshold (0.5)')\n",
    "#\n",
    "# # Formatting\n",
    "# # plt.title(f'Ice Model Performance - Lake ID: {lake_id}, Winter {winter_year}/{winter_year+1}')\n",
    "# plt.ylabel('Ice Probability', fontsize=16)\n",
    "# plt.ylim(-0.03, 1.03)\n",
    "# plt.yticks([0, 0.5, 1], fontsize=14)\n",
    "# # Format x-axis with month labels\n",
    "# plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%b'))\n",
    "# plt.gca().xaxis.set_major_locator(mdates.MonthLocator())\n",
    "# plt.gcf().autofmt_xdate()\n",
    "#\n",
    "# # Add legend\n",
    "# plt.legend(fontsize=16, loc='lower center')\n",
    "# plt.grid(True, alpha=0.3)\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ],
   "id": "61d96d59bdfba817",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Run the Model: prediction",
   "id": "4867a2d864e1315e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-30T00:43:34.971814Z",
     "start_time": "2025-08-30T00:43:32.163387Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Prepare data for prediction (only for rows where ice_binary is None)\n",
    "prediction_rows = complete_df['ice_binary'].isna()\n",
    "print(f\"Total rows needing prediction: {prediction_rows.sum()}\")\n",
    "# Extract features for prediction\n",
    "X_to_predict = complete_df.loc[\n",
    "    prediction_rows, ['elevation', 'latitude', 'area', 'afdd', '$ T_{wk,L} $']]\n",
    "# Check for and drop rows with NaN values\n",
    "rows_with_nans = X_to_predict.isna().any(axis=1)\n",
    "if rows_with_nans.any():\n",
    "    nan_count = rows_with_nans.sum()\n",
    "    print(f\"Dropping {nan_count} rows with NaN values ({nan_count / len(X_to_predict):.2%} of prediction set)\")\n",
    "    # Create a clean version with no NaNs\n",
    "    X_to_predict_clean = X_to_predict.dropna()\n",
    "    # Update our prediction_rows mask to exclude rows we're dropping\n",
    "    valid_prediction_indices = X_to_predict_clean.index\n",
    "    prediction_rows_clean = prediction_rows.copy()\n",
    "    prediction_rows_clean[~prediction_rows_clean] = False  # Keep False values as False\n",
    "    prediction_rows_clean[prediction_rows] = prediction_rows.index.isin(valid_prediction_indices)  # Update True values\n",
    "    # Scale and predict only on clean data\n",
    "    scalar = StandardScaler()\n",
    "    X_scaled = scaler.transform(X_to_predict_clean)\n",
    "    print(f\"Predicting ice status for {len(X_scaled)} rows...\")\n",
    "    predictions = lake_ice_model.predict_proba(X_scaled)[:, 1]\n",
    "\n",
    "    # Assign predictions only to the clean subset of rows\n",
    "    complete_df.loc[valid_prediction_indices, 'ice_probability'] = predictions\n",
    "    complete_df.loc[valid_prediction_indices, 'ice_prediction'] = (predictions > 0.5).astype(int)\n",
    "else:\n",
    "    # No NaNs found, proceed with all rows\n",
    "    X_scaled = scaler.transform(X_to_predict)\n",
    "    print(f\"Predicting ice status for all {len(X_scaled)} rows...\")\n",
    "    predictions = lake_ice_model.predict_proba(X_scaled)[:, 1]\n",
    "    complete_df.loc[prediction_rows, 'ice_probability'] = predictions\n",
    "    complete_df.loc[prediction_rows, 'ice_prediction'] = (predictions > 0.5).astype(int)\n",
    "\n",
    "# For observed data points, copy ice_binary to ice_prediction and set probability accordingly\n",
    "observed_rows = ~prediction_rows  # \"equals not the prediction rows...\"\n",
    "complete_df.loc[observed_rows, 'ice_prediction'] = complete_df.loc[observed_rows, 'ice_binary']\n",
    "complete_df.loc[observed_rows, 'ice_probability'] = complete_df.loc[observed_rows, 'ice_binary']\n",
    "\n",
    "# Fill any remaining NaN values in the prediction columns\n",
    "missing_predictions = complete_df['ice_prediction'].isna()\n",
    "if missing_predictions.any():\n",
    "    print(f\"Note: {missing_predictions.sum()} rows still have missing predictions (dropped due to NaNs)\")\n",
    "    # You can choose to fill these with a default value or leave them as NaN\n",
    "    complete_df.loc[missing_predictions, 'ice_prediction'] = -1  # Special code for \"couldn't predict\"\n",
    "    complete_df.loc[missing_predictions, 'ice_probability'] = -1\n"
   ],
   "id": "60fe3412ab3253b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows needing prediction: 7012240\n",
      "Dropping 1881 rows with NaN values (0.03% of prediction set)\n",
      "Predicting ice status for 7010359 rows...\n",
      "Note: 1881 rows still have missing predictions (dropped due to NaNs)\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Generate Ice Metrics Dataset for visualization",
   "id": "b0fad7e3d5c418b4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-30T00:43:46.037530Z",
     "start_time": "2025-08-30T00:43:43.715593Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Calculate Ice Phenology Metrics with Confidence Scores\n",
    "def calculate_ice_metrics(df, prob_threshold=0.5):\n",
    "    \"\"\"Calculates ice-on, ice-off, duration, and observation density for each lake and winter.\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame with ice_prediction, ice_probability, lake_id, winter_year, winter_doy, and data_type columns.\n",
    "        prob_threshold (float): Probability threshold for determining ice presence.\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with ice metrics (ice_on_doy, ice_off_doy, ice_duration, obs_density).\n",
    "    \"\"\"\n",
    "    # Filter ice data based on the probability threshold\n",
    "    ice_data = df[df['ice_probability'] >= prob_threshold].copy()\n",
    "    # Group by lake and winter to find ice-on and ice-off dates\n",
    "    ice_on = ice_data.groupby(['lake_id', 'winter_year'])[\n",
    "        'winter_doy'].min().reset_index()\n",
    "    ice_off = ice_data.groupby(['lake_id', 'winter_year'])[\n",
    "        'winter_doy'].max().reset_index()\n",
    "\n",
    "    # Count number of days classified as ice for each lake and winter year\n",
    "    ice_duration = ice_data.groupby(['lake_id', 'winter_year']).size().reset_index(name='ice_duration')\n",
    "    # Merge ice-on and ice-off dates\n",
    "    ice_metrics = pd.merge(ice_on, ice_off, on=['lake_id', 'winter_year'],\n",
    "                           suffixes=('_on', '_off'))\n",
    "    # Add ice duration to metrics\n",
    "    ice_metrics = pd.merge(ice_metrics, ice_duration, on=['lake_id', 'winter_year'])\n",
    "    # Calculate observation density\n",
    "    total_days = df.groupby(['lake_id', 'winter_year'])[\n",
    "        'winter_doy'].count().reset_index()\n",
    "    observed_days = df[df['data_type'] == 'Observed'].groupby(['lake_id', 'winter_year'])[\n",
    "        'winter_doy'].count().reset_index()\n",
    "    # Merge total_days and observed_days\n",
    "    obs_density = pd.merge(total_days, observed_days, on=[\n",
    "        'lake_id', 'winter_year'], suffixes=('_total', '_observed'), how='left')\n",
    "    obs_density['obs_density'] = obs_density['winter_doy_observed'].fillna(\n",
    "        0) / obs_density['winter_doy_total']\n",
    "    # Merge ice metrics with observation density\n",
    "    ice_metrics = pd.merge(ice_metrics, obs_density[[\n",
    "        'lake_id', 'winter_year', 'obs_density']], on=['lake_id', 'winter_year'])\n",
    "    # Calculate confidence score\n",
    "    ice_metrics['confidence'] = ice_metrics['obs_density']\n",
    "\n",
    "    return ice_metrics\n",
    "# Apply the function to calculate ice metrics\n",
    "################### RESULTS ###################################\n",
    "ice_metrics_df = calculate_ice_metrics(complete_df)\n",
    "###############################################################"
   ],
   "id": "2710a65c4afa42f3",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-30T00:45:03.273108Z",
     "start_time": "2025-08-30T00:45:03.133941Z"
    }
   },
   "cell_type": "code",
   "source": "ice_metrics_df.to_csv(r'USER_INPUT', index=False) # save the output CSV",
   "id": "802ee540fd76f836",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b7ca3f6351a3d033"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
